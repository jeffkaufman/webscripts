[["Randy", "https://plus.google.com/102251509192760989541", "gp-1498485362487", "<p>I'm very curious what you find.  \n<br>\n<br>\nI was part of a seminar/working group on catastrophic risk Bruce Schneier did at Harvard a couple of years back, and we spent two sessions exploring AI risk and came to the conclusion that it wasn't currently worth worrying about. The basic conclusion was that the devil was in the details--if you looked at the field from a distance, the chances of Skynet showing up seemed large, but if you dug into what AI actually meant, we were a long ways away from that place along several different axes, most of which were ignore by the broad-brush folks.\n<br>\n<br>\n(The actual, more detailed conclusion: Apocalyptic terminator-like scenarios weren't worth worrying about, but we did think that there was very real risk of first mover advantage in machine learning leading to a substantial increase in the corporate power of the first movers, which could lead in the general direction of corporate dystopia.   I don't consider that a catastrophic risk, and it bothered me less than many other folks in the room because we (humanity) has been dealing to a better or worse extent with excess corporate power for a couple of centuries now, but it was a risk we identified).  \n<br>\n<br>\nIf you'd like me to dig up the articles Schneier put together for us to read and discuss, I'd be happy to.  </p>", 1498485362], ["Thomas", "https://plus.google.com/110993380381592315078", "gp-1498564331323", "<p>I work in ML, and I think AI risk is worth working on now.  I would be happy to talk more, or answer questions, if that would help.</p>", 1498564331], ["Brendan", "https://plus.google.com/100334584094940516862", "gp-1498593807659", "<p>I'm really interested to see what you find. I'm in the camp of \"AI safety is important but current AI safety research seems worthless\", and I suspect that's common among people who have actually done ML. Philosophy papers about decision theory won't be very useful if we end up training AI. If we end up actually programming AI (and not just training it), then we'll likely know a lot more about how minds work at that time than we do now, so speculation today is probably not worth the effort.</p>", 1498593807]]