[["Danner", "https://plus.google.com/114987071963782993407", "gp-1320240716412", "<p>The time estimation doesn't include technological advances, 250 years might be reasonable, but given the logarithmic scale of technology advances, the amount of technology that fits under that curve is ever advancing, and getting to the point of simulating the entire brain will go from being unfeasible to completely reasonable in a short span of time, i'd guess under 25(grounding?) years...as to when those 25 years are, I don't have a guess yet.</p>", 1320240716], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1320247549511", "<p>@<a href=\"https://plus.google.com/114987071963782993407\">Danner</a>\n \"getting to the point of simulating the entire brain will go from being unfeasible to completely reasonable in a short span of time, i'd guess under 25 years\"\n<br>\n<br>\n25 years is about a 10000x speedup with moore's law. So I agree with you: that's a reasonable time to take simulation from intractable to tractable in terms of raw computation, storage, and maybe even data collection.\n<br>\n<br>\nOn the other hand, the nematode simulation has been tractable in all these ways for 25 years, but we've made slow progress. Emulation turns out to be really hard. I'm not sure this kind of progress is affected much by a moore's law style exponential improvement.</p>", 1320247549], ["Bronwyn", "https://plus.google.com/112209325452034727224", "gp-1320266057191", "<p>A big part of the problem with emulation here is knowing how much abstraction is acceptable.  Knowing the physical wiring diagram of neurons is really hard (though possible in extremely simple systems), but that's just a start.  You almost certainly also have to know the general response properties of each neuron as well as the sign and strength of the synapses.  Do we also have to know and simulate the biophysical dynamics of various neurotransmitters, ion channels etc?  What about cells in the brain that aren't neurons (people are starting to think glial cells might do more than we thought, the structure of blood vessels etc might be important)? We are really far away from knowing enough about the brain to be able to answer that question, I think.\n<br>\n<br>\nPersonally I think the limiting factor would be the ability to record the brain you wish to simulate.  Current brain imaging technologies don't get us anywhere remotely near the level of detail you would need.  I don't think it's a matter of incremental improvements on current technology - we would need a pretty radical technological revolution.  It's hard to predict when or if such things will happen.  But maybe I just say that because I work in brain imaging and like to think of it as a hard problem.  To me, developing the necessary raw computational power seems much more within our grasp (though of course non-trivial).</p>", 1320266057], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1320267287391", "<p>@<a href=\"https://plus.google.com/112209325452034727224\">Bronwyn</a>\n \"Current brain imaging technologies don't get us anywhere remotely near the level of detail you would need\"\n<br>\n<br>\nI'm not sure.  Ken Hayworth [1] claimed at a talk I was at that if we were willing to spend ungodly amounts of money we would be able to make a nanoscale (I think &lt;30nm) imaging pass over the entire brain.  I remember some of the details from the talk, but can't find the slides online.  But this still waits on something able to plasticize the brain, perhaps using the vascular system, and knowing how to interpret the output images to read off everything we need (keeping all the raw images would be an prohibitively huge amount of data).\n<br>\n<br>\n\"Do we also have to know and simulate the biophysical dynamics of various neurotransmitters, ion channels etc? What about cells in the brain that aren't neurons?\"\n<br>\n<br>\nThis is important.\n<br>\n<br>\n[1] I quoted him in the blog post.  Also, see the ATLUM project: \n<a href=\"http://www.mcb.harvard.edu/lichtman/ATLUM/ATLUM_web.htm\">http://www.mcb.harvard.edu/lichtman/ATLUM/ATLUM_web.htm</a></p>", 1320267287], ["BDan", "https://plus.google.com/103775592027106438640", "gp-1320268758124", "<p>That sounds like something that could only be performed on a dead brain, though.  Human brains aren't physically identical like nematode brains, so to simulate the consciousness of any individual we would need a way to perform the scan while they were still alive.\n<br>\n<br>\nI wouldn't worry about the amount of data, though, given that storage capabilities increase exponentially with time as well.</p>", 1320268758], ["Bronwyn", "https://plus.google.com/112209325452034727224", "gp-1320269146922", "<p>From a very quick scan of the web link you posted above, it looks like what he's talking about is strictly structural.  Structure does not equal function - you even quote Hayworth saying that: \"we may know the connectivity but we don't know even the sign of many synapses\".  He seems to be claiming later on that if we had a nanoscale image of the structure we could relatively easily learn how to infer functional properties.  It's possible that with enough physical detail and enough knowledge that's true, but I think I'm less optimistic than he is (in the theoretical limit of spatial scale and knowledge, I imagine it is true, but it might end up being easier or practically necessary to incorporate functional information).  Anyway, \nif\n it is true that we only need structural information, then I concede that we're just looking at large improvements to current technology (plus lots of neuro knowledge).  If we need functional data as well, then I stand by my original statement.  In general in neuroimaging there are tradeoffs between spatial and temporal scale and resolution.  If you want really fine spatial resolution of large numbers of neurons you lose all temporal resolution and therefore all functional information.  We would have to figure out some way to get past that to be able to get detailed structure and function from the complete brain of a single individual.\n<br>\n<br>\n* (At much larger spatial scales people have shown a dissociation between functional and structural connectivity, but that's really a different question and probably has to do with imaging resolution, indirect connections etc.  Honey et al, PNAS, 2009)</p>", 1320269146], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1320272160066", "<p>@<a href=\"https://plus.google.com/103775592027106438640\">BDan</a>\n \"to simulate the consciousness of any individual we would need a way to perform the scan while they were still alive\"\n<br>\n<br>\nOr you could plasticize someone's brain at death before the vascular system decomposes and then scan that. I don't see why the person still needs to be alive.\n<br>\n<br>\n\"I wouldn't worry about the amount of data, though, given that storage capabilities increase exponentially with time as well.\"\n<br>\n<br>\nWe're talking about a lot of data.  A full brain scan at 5nm resolution (5 cubic nm per pixel, layers 5nm apart) would be 10^22 pixels.  At 1 byte per pixel this is still a lot of doublings away.</p>", 1320272160], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1320277441525", "<p>@<a href=\"https://plus.google.com/112209325452034727224\">Bronwyn</a>\n \"If you want really fine spatial resolution of large numbers of neurons you lose all temporal resolution and therefore all functional information\"\n<br>\n<br>\nNot if the person's brain has been basically turned to plastic.  There's no sense of \"temporal resolution\" if you plan to slice a brain into tiny tiny strips and spend a decade running an SEM over it.\n<br>\n<br>\n\"what he's talking about is strictly structural\"\n<br>\n<br>\nYes.  If by 'functional' you mean \"we need to be able to interact with the specific individual's brain while it's still running\", that's probably not practical.</p>", 1320277441], ["Bronwyn", "https://plus.google.com/112209325452034727224", "gp-1320293644022", "<p>Yes, that's what I mean by functional.  If a person's brain is \"turned to plastic\" you can no longer get any functional information directly.  The question I was getting at above was: \n<br>\nWhich will come first (if both are possible)?\n<br>\n<br>\n(a) Sufficiently complete understanding of neural dynamics and their relation to structure such that we can infer \nall\n functional characteristics from static post-mortem imaging. This would require a very detailed low-level knowledge of cell function/disfunction.  Perhaps we would need to know other information about a person (hormone levels, input filters through sensory systems, etc).  What level of molecular detail would your brain preservative preserve?  What would the preservation process destroy?  What would be the necessary spatial scale to be able to visually determine all function or non-function?\n<br>\nPerhaps more importantly, it's unclear to me that one could 'restart' a person without recreating the instantaneous state of activity from some point in their life.  Even if you had all the geometry and biophysics right, could you just input some random (or even heuristically determined) activation and get the whole system going again the way it was before?  \n<br>\n<br>\n(b) Technological revolution that allows \nin vivo\n imaging of a qualitatively different kind than we have now.\n<br>\n<br>\nI'm tempted to guess (b).  But that's just me.  I think the answers to these sorts of questions could be different if you just want to simulate a human-like brain as opposed to a particular human (with memories, personality etc intact).  The nematode simulations you discuss are really more akin to the first thing.</p>", 1320293644], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1320324237675", "<p>@<a href=\"https://plus.google.com/112209325452034727224\">Bronwyn</a>\n I think the questions you are asking in (a) are critical.  The 2008 roadmap [1] from the FHI attempts to answer some of them, and I'm going to attempt to \nimpersonate\n the roadmap in answering your questions.  My views are closer to yours than to the roadmap's, but the people who wrote the roadmap have done a lot of research into this.\n<br>\n<br>\n\"Perhaps we would need to know other information about a person\"\n<br>\n<br>\nWe're not sure what level of detail will be needed, but you can see on p13 that we're considering a very wide range of scales that might be required.  The most likely ones, determined by consensus among brain emulation researchers at our 2007 conference, are levels 4-6 on this table [2].  We would need the connectome, firing properties, firing state, dynamical synaptic states, ion channels, ion state, ion concentrations, currents, voltages, and modulation states.\n<br>\n<br>\nWe talk through this in a lot of detail on pages 31-38.\n<br>\n<br>\n\"What would be the necessary spatial scale to be able to visually determine all function or non-function?\"\n<br>\n<br>\nOn p14 we talk about how in order to achieve states 4-6 we would need about 5x5x50 nm scanning resolution.  This is very high, higher than this image [3], which already shows a lot of cell detail.  As we wrote on p54, however, if the required level of simulation is at level 5 or above (see [2]) we appear to have already achieved the resolution requirements and the remaining problem is in data/tissue management and scaling up methods to handle large brains. Using KESM or ATLUM it should be possible in the very near future to construct a detailed connectome of the brain, in particular the exact interconnections of cortical minicolumns.\n<br>\n<br>\nIf the required level for WBE is below level 5, increases of imaging resolution are of relatively limited use. Rather, we need modalities that enable mapping the proteins, possibly their states and the presence of metabolites or RNA transcripts. This would pose a major research challenge, although it is in line with much research interest today examining the biophysics of cells.\n<br>\n<br>\n\"What level of molecular detail would your brain preservative preserve? What would the preservation process destroy?\"\n<br>\n<br>\nThe preservative process is not very far along.  Eventually we would likely need to use the vascular system, but we don't do that now.  Currently we use several steps, first replacing the water, then in two more passes getting in the plastic, staining with osmium tetroxide (a heavy metal stain) as we go.  It's not clear whether this could be scaled up to a whole brain.  The level of detail you end up with is quite good, however, as you can see in images like [3].\n<br>\n<br>\n\"it's unclear to me that one could 'restart' a person without recreating the instantaneous state of activity from some point in their life. Even if you had all the geometry and biophysics right, could you just input some random (or even heuristically determined) activation and get the whole system going again the way it was before?\"\n<br>\n<br>\np37: The methods for creating the necessary data for brain emulation discussed in this paper deal with just the physical structure of the brain tissue, not its state of activity. Some information such as working memory may be stored just as ongoing patterns of neural excitation and would be lost. Similarly, information in calcium concentrations, synaptic vesicle depletion, and diffusing neuromodulators may be lost during scanning. A likely consequence would be amnesia of the time closest to the scanning.\n<br>\n<br>\nHowever, loss of brain activity does not seem to prevent the return of function and personal identity. This is demonstrated by the reawakening of coma patients, and by cold water near\u2010 drowning cases in which brain activity temporarily ceased due to hypothermia\n<br>\n<br>\n\"Technological revolution that allows in vivo imaging of a qualitatively different kind than we have now\"\n<br>\n<br>\nThis sounds really hard.  You're doing it in vivo because you need information that is not kept when the brain is plasticized, right?  Because the information is quickly variable?  Then you need really high temporal and spacial resolution at once, which you had suggested before was not practical.\n<br>\n<br>\n<br>\n[1] \n<a href=\"http://www.fhi.ox.ac.uk/Reports/2008-3.pdf\">http://www.fhi.ox.ac.uk/Reports/2008-3.pdf</a>\n<br>\n[2] \n<a href=\"http://i.imgur.com/T9J9Q.png\">http://i.imgur.com/T9J9Q.png</a>\n<br>\n[3] \n<a href=\"http://i.imgur.com/4yR1w.png\">http://i.imgur.com/4yR1w.png</a></p>", 1320324237], ["Bronwyn", "https://plus.google.com/112209325452034727224", "gp-1320328713012", "<p>I guess I don't really think either is practical at the moment, and they are both really hard. The first one is easier in a way because we can sort of see where to work to make incremental progress.  The second one we can only sort of say \"uh, well, I dunno.  but maybe we'll learn something new about basic physics or something\".\n<br>\nNow I've gotten to thinking which would make a better scifi story.  Is it more interesting to create a world where you have to die to be computerized (possible murder and mayhem) or where you don't (multiple copies of the same person)?</p>", 1320328713], ["Neil", "https://plus.google.com/107204461218920856688", "gp-1320814115622", "<p>Heard about this recently: \n<a href=\"http://www.neuroinformatics2011.org/abstracts/open-connectome-project-reverse-engineering-the-brain-one-synapse-at-a-time\">http://www.neuroinformatics2011.org/abstracts/open-connectome-project-reverse-engineering-the-brain-one-synapse-at-a-time</a>\n. Given that is uses electron microscopy, it's not on a living brain which is problematic for uploading people. One point to note: a cubic millimeter occupies roughly 1 petabyte. The hum brain is roughly 1.1 million mm^3. That's a heck of a lot of data.</p>", 1320814115], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1320840169606", "<p>@<a href=\"https://plus.google.com/107204461218920856688\">Neil</a>\n While I think uploading a person is a long way off, I think electron microscopy on recently dead brains is the most likely way we'd collect the data. Anything with a living person would require major physics breakthroughs, while SEM over the whole brain is just a massive scaling up of technology we mostly already have.\n<br>\n<br>\nWhile a million petabytes is a lot of data, hard drive capacity has been doubling annually for thirty years. If it does this for another thirty then that would be like a gigabyte is today.</p>", 1320840169], ["Neil", "https://plus.google.com/107204461218920856688", "gp-1320850985741", "<p>Drive speed is increasing more slowly, but point taken. If we keep going at this rate it won't be that long until that sort of digitizing will be possible. Whether that level of detail will be good enough is another question.</p>", 1320850985], ["Bronwyn", "https://plus.google.com/112209325452034727224", "gp-1320853029507", "<p>As the abstract Neil linked to says, \"Clearly, while even collecting this type of data is an enormous task, interpreting and analyzing the data is far more difficult.\"  \n<br>\nIt seems like most of this discussion has focused on the engineering side of things instead of the knowledge side of things.  I think that a \nreally\n large imaging breakthrough with a large increase in knowledge might be more likely than a large increase in imaging technology with a \nreally\n large knowledge breakthrough.  I'm not sure I can justify the bias, but I instinctively feel that innovations in technology tend to outpace breakthroughs in specific areas of (natural) science knowledge.  Maybe because technology is generally much more transferrable between fields than, say, knowledge about low level neuroscience.</p>", 1320853029], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1320854359985", "<p>@<a href=\"https://plus.google.com/112209325452034727224\">Bronwyn</a>\n \"innovations in technology tend to outpace breakthroughs in specific areas of (natural) science knowledge\"\n<br>\n<br>\nI think these calculations are already taking this into account.  Getting 30 doubling of storage capacity in 30 years is only possible with continued rapid technological innovation.  Scaling up SEM to a whole brain likewise.</p>", 1320854359], ["Bronwyn", "https://plus.google.com/112209325452034727224", "gp-1320854725029", "<p>Which is why I'm not that skeptical that we'll be able to scan a whole brain in the way you propose at some point. \n<br>\nAll I'm saying is that I don't think we'll have the necessary scientific knowledge to get a simulation from your scan until far enough in the future that I don't think we can predict even the sort of technology we will be dealing with.</p>", 1320854725], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1320856508974", "<p>@<a href=\"https://plus.google.com/112209325452034727224\">Bronwyn</a>\n ok, then I agree with you.</p>", 1320856508]]