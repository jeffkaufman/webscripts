[["David&nbsp;Chudzicki", "https://plus.google.com/106120852580068301475", "gp-1341857597395", "<p>Another example pointing to preferences as more intuitively valuable than utility [edit: poor word choice, see Jeff below]?</p>", 1341857597], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1341858459564", "<p>@<a href=\"https://plus.google.com/106120852580068301475\">David&nbsp;Chudzicki</a>\n\u00a0I don't see what about this suggests valuing preferences over happiness. \u00a0Explain?\n<br>\n<br>\n(Assuming you mean \"happiness\" instead of \"utility\". \u00a0Generally whatever you value gets called \"utility\", and could be happiness, preference satisfaction, or something more complex.)</p>", 1341858459], ["David&nbsp;Chudzicki", "https://plus.google.com/106120852580068301475", "gp-1341858941129", "<p>Re utility yeah, oops. Didn't mean utility. Maybe happiness, though if we're starting to generalize away form \"people\" to arbitrary computational processes that may be \"valuable\" in some way, 'happiness' starts to sound odd.\n<br>\n<br>\nRe preferences-- it seems relevant that that particular process (the uploaded secluded person) has preferences for remaining alive. Destroying it and replacing it with other new ones (even if they're happier and/or there are more of them) doesn't seem good.</p>", 1341858941], ["Allison", "https://plus.google.com/103741579182942078941", "gp-1341858970362", "<p>I could easily write a chatterbot that essentially says two things: \"leave me alone\" and \"don't shut me down.\" \u00a0If you try and cull an EM down, you might end up with something equivalent in value to that chatterbot. \u00a0How do you even begin to determine value? \u00a0\n<br>\n<br>\nRelatedly, If I were given the choice between reliving the happiest points in my life in emulation, or death, I'd pick death, because living on repeat isn't living, it's remembering.</p>", 1341858970], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1341864379840", "<p>@<a href=\"https://plus.google.com/103741579182942078941\">Allison</a>\n\u00a0\"living on repeat isn't living, it's remembering\"\n<br>\n<br>\nTo be fair, though, remembering can be pleasant too.</p>", 1341864379], ["Allison", "https://plus.google.com/103741579182942078941", "gp-1341866456075", "<p>@<a href=\"https://plus.google.com/103013777355236494008\">Jeff&nbsp;Kaufman</a>\n\u00a0I agree, remembering can be pleasant. \u00a0However, I'd rather make new memories most of the time; growth and change is important to me. \u00a0</p>", 1341866456], ["David&nbsp;Chudzicki", "https://plus.google.com/106120852580068301475", "gp-1341867903311", "<p>@<a href=\"https://plus.google.com/103013777355236494008\">Jeff&nbsp;Kaufman</a>\n\u00a0(\n@<a href=\"https://plus.google.com/103741579182942078941\">Allison</a>\n) exactly! preferences</p>", 1341867903], ["Chris", "https://plus.google.com/117346402173047680184", "gp-1341940961252", "<p>I find it interesting that you consider copying a human's personhood into a computer.\u00a0 It seems much more likely to me that we'll get artifical people that aren't copies of humans much sooner and I don't see any reason why they would have any less value than a copy of a human.</p>", 1341940961], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1341946222384", "<p>@<a href=\"https://plus.google.com/117346402173047680184\">Chris</a>\n\u00a0Reasoning about a person copied onto a computer is much easier, and I would expect any conclusions to cross-apply to non-human-derived artificial people.</p>", 1341946222], ["Chris", "https://plus.google.com/117346402173047680184", "gp-1341955147344", "<p>I think if your arguments depend on the fact that the artificial person is a copy of a human than they won't cross-apply by default.\u00a0 I guess I could see it.\u00a0 You need to argue two things.\u00a0 Firstly that someone made of silicon is not fundamentally different from someone made of carbon and then that someone who thinks differently than you.\n<br>\n<br>\nHowever, whether it makes the reasoning easier or not, I think that artificial people who aren't copies of humans will come earlier.\n<br>\n<br>\nI'm also tempted to point out that you said a copy of a person and not of a human.\u00a0 I'm not sure if this reveals a bias in our language that person = human or whether it was intentional since once they're copied, they won't quite be human any more.</p>", 1341955147], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1342785629594", "<p>@<a href=\"https://plus.google.com/105024376133521186824\">Lucas</a>\n\u00a0My computer example included that you had some em that wanted to be cut off from the rest of the world and disable input and output. \u00a0Once you have this, you can run as many copies as you want at whatever speeds you want and I think simple\u00a0arithmetic\u00a0applies to their morality (2x speed means twice as good, half as many is half as good.) \u00a0Not having any communication is key for this simplification.</p>", 1342785629], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1342873792306", "<p>@<a href=\"https://plus.google.com/105024376133521186824\">Lucas</a>\n\u00a0Right. \u00a0They might not be happy, but 2x speed means whatever they are it matters twice as much.</p>", 1342873792]]