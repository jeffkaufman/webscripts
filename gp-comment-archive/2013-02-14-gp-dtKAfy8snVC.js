[["David&nbsp;German", "https://plus.google.com/111229345142780712481", "gp-1360907144020", "<p>\"Then, assuming we're really in thought-experiment land, where we can be completely sure that no one will ever know we robbed from one charity to give to another, I would say it would be wrong not to take the money.\"\n<br>\n<br>\nJust to confirm: in your opinion, the only reason not to steal (for the benefit of your preferred charities) is the risk of getting caught?</p>", 1360907144], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1360930460246", "<p>@<a href=\"https://plus.google.com/111229345142780712481\">David&nbsp;German</a>\n\u00a0\"in your opinion, the only reason not to steal (for the benefit of your preferred charities) is the risk of getting caught?\"\n<br>\n<br>\nStealing has all sorts of harmful and corrosive effects. \u00a0These effects, as far as I can tell, come from people being aware of the stealing. \u00a0So while in practice I think stealing is wrong (has bad effects), in a thought experiment where those bad effects are specified not to happen (no one will know) I don't think its wrong.</p>", 1360930460], ["Chris", "https://plus.google.com/112938759017605010116", "gp-1360933057252", "<p>Performing act-consequentialism directly leads to reliably worse outcomes than other methods\u00b9, so most (all?) utilitarians adopt some form of rule-consequentialism instead, whereby you follow the act-consequentialism-inspired rules that would lead to the best outcomes when followed by almost everyone everywhere. \u00a0Stealing is unlikely to be permitted by those rules.\n<br>\n<br>\nSo, I wouldn't steal even in this thought experiment, because my decision procedure exists separately to my own calculations of the moral worth of the act (which I'm frankly dubious of most of the time). \u00a0Toby Ord's written a little more about this: \u00a0\n<a href=\"http://www.amirrorclear.net/academic/papers/decision-procedures.pdf\">http://www.amirrorclear.net/academic/papers/decision-procedures.pdf</a>\n<br>\n<br>\n\u00b9: page 8 of Ord's PDF</p>", 1360933057], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1360950272547", "<p>@<a href=\"https://plus.google.com/112938759017605010116\">Chris</a>\n\u00a0I agree that we can end up with better outcomes by applying some rules that don't on their face sound like they're always optimal. \u00a0Which is why in practice I treat \"don't steal\" as a hard rule even though in some thought experiments that posit extreme certainty I think stealing could lead to better outcomes. \u00a0But \"the act-consequentialism-inspired rules that would lead to the best outcomes when followed by almost everyone everywhere\" seems much too weak. \u00a0I think something like \"follow normal moral rules most of the time, but when the stakes are high try to work through the consequences of your choices as best as you can\" might be better?</p>", 1360950272], ["Chris", "https://plus.google.com/112938759017605010116", "gp-1360966867949", "<p>@<a href=\"https://plus.google.com/103013777355236494008\">Jeff&nbsp;Kaufman</a>\n\u00a0Adding a theory of \"moral emergency\" seems intuitive, but I think it adds more questions than it answers. \u00a0First, how do we know when we're in a moral emergency? \u00a0This is extremely subjective; as Yvain points out, \"Some people would steal a PlayStation, and think up some bogus moral justification for it later\". \u00a0Societal trust depends pretty heavily on us agreeing ahead of time which type of events are moral emergencies and being able to predict the range of behaviors that others will choose. \u00a0Arguably this is the situation we already have, with democratic laws.\n<br>\n<br>\nMaybe we can work around this problem of humans using subjective and selfish reasoning by limiting our use of moral emergency to cases where we don't personally benefit in any way. \u00a0But now we have another problem, which Ord writes about too: \"A major problem for benevolent calculation concerns the cost of deliberation. If someone is drowning in the river and we pause to calculate, he will likely die. The only way to achieve the best outcome in such a case is through some easily applied rule, such as those found in common-sense morality. While this example is extreme, there are a great many cases in which the costs of the time spent calculating predictably outweigh the benefits.\"\n<br>\n<br>\nOrd's example is assuming that the calculation will even be performed correctly -- I think most humans are terrible with objective statistics even when given infinite time to calculate, and almost all humans will make mistakes when influenced by adrenaline and emotion. \u00a0This isn't the right time to be performing important calculations either.\n<br>\n<br>\nIt's a frustrating paradox, but it seems inescapable that the way to perform acts that have the best outcomes is generally by refusing to calculate the outcomes of acts in real-time.</p>", 1360966867], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1360976675479", "<p>@<a href=\"https://plus.google.com/112938759017605010116\">Chris</a>\n\u00a0I think you're right that under time pressure you're much more likely to make a good decision if you have a moral rule to apply (help people; don't steal; etc) than if you try and think through all the consequences in full right then. \u00a0But what about when we do have time? \u00a0For example, what about when making career choices or picking between charities. \u00a0Those are cases where I think it makes sense to really think through consequences, and not just use quick-to-apply moral rules.</p>", 1360976675], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1360984793072", "<p>(\n@<a href=\"https://plus.google.com/112938759017605010116\">Chris</a>\n\u00a0An example of this kind of \"use act-consequentialism not rule-consequentialism at high stakes when you have good information and time to consider it\" would be the main tragedy of \"City on the Edge of Forever\", which I just happened to watch.)</p>", 1360984793]]