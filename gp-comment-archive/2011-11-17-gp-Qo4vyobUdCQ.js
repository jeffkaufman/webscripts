[["Todd", "https://plus.google.com/112947709146257842066", "gp-1321584453861", "<p>Perhaps I over-adjust for this. I tend to put basically no stock in any single study.</p>", 1321584453], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1321622343822", "<p>George points out (on facebook) that I'm misinterpreting p-values here.  They're not the inverse probability of replication or the probability that results are due to chance, because they ignore prior probability.</p>", 1321622343], ["Frederic", "https://plus.google.com/118156077148469167305", "gp-1321746356814", "<p>There are adjustments to make that eliminate your \"interpretation bias\" problem (the simplest one is the Bonferroni correction). You need to account for the number of hypotheses that you're testing when calculating your p-value; the more hypotheses, the more evidence you need to attain a certain p-value. Of coure, unscrupulous or inept experimenters can ignore this, but it's not some sort of intractable problem from the setup end.</p>", 1321746356]]