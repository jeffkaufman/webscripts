[["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340980146461", "<p>If the extrapolated volition of humanity was to converge to wireheading then it could as well mean that killing all humans and turning the universe into some sort of\u00a0Nozick\u00a0machine would be the right thing to do.\u00a0\n<br>\n<br>\nBut given expected utility maximization I believe that the most likely outcome will be that all resources are going to be devoted to\u00a0prevent the heat death of the universe by trying to hack the matrix or attempt time travel etc.</p>", 1340980146], ["Alex", "https://plus.google.com/100936518160252317727", "gp-1340980843226", "<p>This is precisely why I find utilitarianism to be a useless tool for making many types of moral decisions. Who are we to say that happiness is a metric that can be accurately measured, quantified, and compared, or that a universe with a larger overall happiness * person product is a superior one to a universe with a smaller one, or that it is our moral duty to maximize happiness, or that happiness is even the right metric (or preference-satisfaction, or whatever)? If we follow that path, we inevitably tend toward preposterous conclusions like \"wireheading everyone is our moral obligation\".\n<br>\n<br>\nThe gap between any metric we could conceivably measure and compare and use as a proxy for moral good, and actual moral good (whatever that means), is a chasm that utilitarianism will inevitably fall into.</p>", 1340980843], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340981321713", "<p>@<a href=\"https://plus.google.com/100936518160252317727\">Alex</a>\n\u00a0What reason do we have to suspect that our intuition is better at judging what we \nshould\n do?\n<br>\n<br>\nCalling a conclusion like \n\"wireheading everyone is our moral obligation\"\n preposterous might be similar to calling quantum mechanics \"weird\".</p>", 1340981321], ["Alex", "https://plus.google.com/100936518160252317727", "gp-1340983239908", "<p>@<a href=\"https://plus.google.com/106808239073321070854\">Alexander</a>\n: Quantum mechanics \nis\n weird. That doesn't make it any less valid a model, especially when it happens to have turned out to be such a good one. In the absence of other evidence, though, I'm going to trust a moral system that comes to reasonable conclusions most of the time over one that comes to unreasonable ones.\n<br>\n<br>\nI don't know if simple intuition is better or worse than utilitarianism in this respect. But I do know that if we are going to make sweeping moral judgments like this, we need a more sophisticated model of human behavior than any we currently have. I believe that such a model would reveal that \"happiness\" (a) is not a good metric for evaluating good, and (b) is not even quantifiable or comparable in any way that would make it useful as such a metric. Furthermore I suspect that there are not many better metrics, or alternatively, that any metric that is better is even harder to quantify.</p>", 1340983239], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340983910165", "<p>But what methods do you use to discern reasonable from unreasonable conclusions? Your gut feelings?\n<br>\n<br>\nCorrect me if I misinterpret you. But you seem to be suggesting that you favor exploration over exploitation. In other words, trying to figure out what humans actually want.\n<br>\n<br>\nDoes that mean that the only moral action right now is to either work directly to dissolve human nature or contribute money to that undertaking?\u00a0\n<br>\n<br>\nIf not. On what grounds do you justify doing something else if you believe that our models are inferior to our intuition, which was never honed to make large scale and long term moral judgements?</p>", 1340983910], ["Alex", "https://plus.google.com/100936518160252317727", "gp-1340985383590", "<p>@<a href=\"https://plus.google.com/106808239073321070854\">Alexander</a>\n, I think we have lots of tools for deciding what is moral and what is not, and we should use the best tool that we have at any given time. Improving our tools is an ongoing process -- but we don't need to drop everything right now to build the best tool we can. We do, after all, have other obligations in the meantime.\n<br>\n<br>\nNote that I didn't say that utilitarianism is useless in \nall\n situations -- it's just another tool. If I want to carefully weigh which charity I want to donate to, for example, I might enumerate some of my goals and decide which allocation of my limited resources best satisfies those goals. I just think there are lots of situations where an attempt to apply utilitarianism gives unsatisfactory results, and this is one example.</p>", 1340985383], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340986221943", "<p>@<a href=\"https://plus.google.com/100936518160252317727\">Alex</a>\n\u00a0We might have a lot of tools. But which one should we use? Even rational people vary dramatically when it comes to what is moral and what is not.\n<br>\n<br>\nTake for example GiveWell, the Singularity Institute and mathematician John Baez. They all know each others tools, are all highly rational and know each others arguments. Yet all disagree considerably about what to do.\n<br>\n<br>\nI do not believe that what GiveWell tells me to do, i.e.\u00a0contributing\u00a0money to fight malaria is a moral obligation or the best you can do. I also don't think, as John Baez does, that climate change is the most worrisome problem humanity faces. And I believe that working to make artificial general intelligence safe to humans will increase rather than decrease the chance of a horrible outcome.\n<br>\n<br>\nSo who is right and why? I don't see that the\u00a0problem\u00a0is tractable at all.\n<br>\n<br>\nBut I am curious how you do it, that's why I am asking.</p>", 1340986221], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340986375418", "<p>Addendum: Wireheading and antinatalism seem equally valid to me.\u00a0</p>", 1340986375], ["Alex", "https://plus.google.com/100936518160252317727", "gp-1340988475145", "<p>@<a href=\"https://plus.google.com/106808239073321070854\">Alexander</a>\n:\u00a0we\u00a0each make our own moral judgments, including whether or not to criticize others for their own decisions. Two people having the same set of tools and coming to different conclusions is pretty compelling evidence for moral relativism. I don't think that the question of \"who is right\" is intractable -- but the answer depends on who is asking, their social context, and lots of other things. It seems silly to ask how well an act satisfies some particular metric of \"good\" \nin every situation\n, since that metric might not be the best one for someone else, or for a different situation.\n<br>\n<br>\nI personally use one set of tools in a particular situation; someone else might use different tools, or use different metrics. My decision depends on my social context, my intuitions, my mental state at that moment, my knowledge of other similar situations and outcomes, prejudices, etc. To generalize on that would require detailed understanding of what my mind is doing as I make a decision. Morality is messy.</p>", 1340988475], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1340989230761", "<p>@<a href=\"https://plus.google.com/100936518160252317727\">Alex</a>\n\u00a0\"\u00a0Two people having the same set of tools and coming to different conclusions is pretty compelling evidence for moral relativism.\"\n<br>\n<br>\nAnother\u00a0possibility\u00a0is that one or both are making a mistake.</p>", 1340989230], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340989379513", "<p>@<a href=\"https://plus.google.com/100936518160252317727\">Alex</a>\n\u00a0If morality is subjective then objectively each represented moral position can only have same weight. As deciding which position shall be assigned more weight than some other position would demand objective grounds on which to decide what is right. Which would contradict.\u00a0\n<br>\n<br>\nBut this also means that an\u00a0equilibrium\u00a0of all\u00a0represented positions does constitute an objective foundation for what actions are less wrong than others, for decision theoretic reasons.\n<br>\n<br>\nBut that doesn't really help at all, apart from possibly being philosophically satisfactory, because it is completely intractable and therefore can't be used to make actual decisions.</p>", 1340989379], ["Alex", "https://plus.google.com/100936518160252317727", "gp-1340989682926", "<p>@<a href=\"https://plus.google.com/103013777355236494008\">Jeff&nbsp;Kaufman</a>\n\u00a0but who are we to say? Like, how would we even say that any system we came up with were a better way to judge either conclusion?</p>", 1340989682], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340989993403", "<p>@<a href=\"https://plus.google.com/100936518160252317727\">Alex</a>\n\u00a0I don't think it is possible to decide.\n<br>\n<br>\nSome people claim that an empirical approach would be favorable. Where we learn and adapt. But since our values are not stable all we will end up with is an implementation of our methods rather than a\u00a0methodical implementation of our values:\u00a0\n<a href=\"http://kruel.co/2011/07/22/objections-to-coherent-extrapolated-volition/\">http://kruel.co/2011/07/22/objections-to-coherent-extrapolated-volition/</a></p>", 1340989993], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340990436396", "<p>Take for example expected utility maximization.\u00a0\n<br>\n<br>\nIf you calculate the expected utility of various outcomes you imagine impossible alternative actions. The alternatives are impossible because you already precommited to choosing the outcome with the largest expected utility.\u00a0\n<br>\n<br>\nYou swap your complex values for a certain terminal goal with the highest expected utility, indeed your instrumental and terminal goals converge to become the expected utility formula.\n<br>\n<br>\nIn other words, expected utility maximization seems to be a tool that helps you to realize your values but in the end alters your behavior in such a way that best reflects the\u00a0methodology rather than your\u00a0initial values.\u00a0\u00a0\u00a0</p>", 1340990436], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340990615980", "<p>In other words, if morality is value based and values are not static then morality changes as your values change. Which makes any decision time-inconsistent because initial actions cease to be moral in the long-term.</p>", 1340990615], ["Alex", "https://plus.google.com/100936518160252317727", "gp-1340990969836", "<p>@<a href=\"https://plus.google.com/106808239073321070854\">Alexander</a>\n\u00a0I agree that any methodology for \"discovering\" morality itself influences the act of being moral, and also that morality changes over time, which is why I think it's important to understand the psychological basis for morality -- that is, asking how individuals decide what is moral, rather than merely asking, \"empirically, what is moral?\"</p>", 1340990969], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340991421056", "<p>@<a href=\"https://plus.google.com/100936518160252317727\">Alex</a>\n\u00a0But if there was a way to determine how individuals decide what is moral, then wouldn't that also answer the question what is\u00a0empirically\u00a0moral?\n<br>\n<br>\nAnd if you believe that to be possible, then shouldn't it be your moral obligation to figure out what is moral to subsequently implement it?\n<br>\n<br>\nAnd if you figured out a decision procedure that reflects how humans decide what is moral. Then in what sense would it be the \nright\n thing to do?\n<br>\n<br>\nJust because evolution implemented such a procedure does not mean that we wouldn't be better off following a different procedure.</p>", 1340991421], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340991572225", "<p>I should elaborate on my last comment.\n<br>\n<br>\nWhat I meant is that if we figured out an algorithm that could judge what is moral that does not mean that it wouldn't contradict by judging actions to be moral that lead up to immoral world states.</p>", 1340991572], ["Alexander", "https://plus.google.com/106808239073321070854", "gp-1340992248986", "<p>What I mean is that, from a\u00a0intuitive\u00a0human moral perspective, consequentialism is\u00a0contradictory. Means do not justify ends given the moral intuition of many people. Yet this does contradict the judgement of the results, where morally superior world states are the result of immoral actions.\n<br>\n<br>\nAnd the father you go down the road of logical implications the more you keep contradicting yourself.\u00a0\n<br>\n<br>\nIn the end, is there a \nright\n thing to do? I don't see how there could be.</p>", 1340992248]]