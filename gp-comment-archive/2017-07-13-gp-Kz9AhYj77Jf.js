[["Randy", "https://plus.google.com/102251509192760989541", "gp-1499960041700", "<p>I think if you consider AGI in scope, the issue is that such an entity is likely to be both \nvery\n powerful and \nvery\n alien in its goals, and that image is scary without there having to be a specific concern.  I basically agree with this concern, I just think we're nowhere near AGI.\n<br>\n<br>\nI think it's more rational to consider the likely pathway to some dangerous point to be advances in some particular subset of the skills we lump under intelligence.  I think people model this as \"Omnipotent with simplistic goals\", which I think does very reasonably lead to the paperclip problem--any simplistic goal coupled with arbitrary power to achieve that goal is likely to result in ugly collateral consequences.  However, I think that modeling advances in particular portions of intelligence as \"Omnipotent with simplistic goals\" is, well, very simplistic :-}.  \n<br>\n<br>\nI think a much better model is that such advances will result in increased power for some particular set of humans, and while that's concerning, it's very far from an existential risk.\n<br>\n<br>\nHaving said all that, how can I comment on your blog rather than on your G+ posts?</p>", 1499960041], ["Todd", "https://plus.google.com/111043209563321619471", "gp-1499960951361", "<p>I think the only way to achieve AGI is for the system to acquire all five senses of it's physical world.  That's how humans achieve their intelligence - they build billions of small learning achievements on each other to create an internal model of the physical world.\n<br>\n<br>\nThis happens in the emotional world as well.  So, why wouldn't such a system that did need to build all the foundations first, end up with learning the same morals that humans do?  Of course, this doesn't mean they wouldn't have different instincts and internal goals of survival.  But if our attitude toward them gave them praise for doing good things all around, I think that would become their driving goal set.  \n<br>\n<br>\nProblem solved.  ...unless we instilled a goal of wealth in them!</p>", 1499960951], ["David&nbsp;German", "https://plus.google.com/111229345142780712481", "gp-1499962953967", "<p>Except for the pharmaceutical example, these all presume sophisticated robotics that I think is roughly as difficult and distant as AGI itself. </p>", 1499962953], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1499964386832", "<p>@<a href=\"https://plus.google.com/102251509192760989541\">Randy</a>\n \"Having said all that, how can I comment on your blog rather than on your G+ posts?\"\n<br>\n<br>\nCommenting on the G+ or FB post does comment on the blog: \n<a href=\"http://www.jefftk.com/p/examples-of-superintelligence-risk#gp-1499960041700\">http://www.jefftk.com/p/examples-of-superintelligence-risk#gp-1499960041700</a></p>", 1499964386], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1499964519907", "<p>@<a href=\"https://plus.google.com/111229345142780712481\">David&nbsp;German</a>\n \"these all presume sophisticated robotics that I think is roughly as difficult and distant as AGI itself\"\n<br>\n<br>\nThese are mostly talking about \"what if we get to superhuman levels of intelligence with incredibly simplistic goals\", and I think they're figuring that at high enough levels of intelligence sophisticated robots aren't that much of a challenge?</p>", 1499964519], ["David&nbsp;German", "https://plus.google.com/111229345142780712481", "gp-1499968969137", "<p>\"I think they're figuring that at high enough levels of intelligence sophisticated robots aren't that much of a challenge?\"\n<br>\n<br>\nI think that's an error. While I won't claim deep expertise, I did work full-time in robotics for more than a year. The robots envisioned in those examples are science fiction relative to the current state of the field.\n<br>\n<br>\nIf you're going to entertain doomsday scenarios in which an AGI directly manipulates the physical world though electromechanical devices, please consider expanding your panels to include robotics experts as well as ML experts. </p>", 1499968969], ["Jeff&nbsp;Kaufman", "https://plus.google.com/103013777355236494008", "gp-1499969444912", "<p>@<a href=\"https://plus.google.com/111229345142780712481\">David&nbsp;German</a>\n I think if robots would be a blocker, manipulating humans to do what they want (either by convincing or by earning money and then paying people) could go a long way</p>", 1499969444], ["Randy", "https://plus.google.com/102251509192760989541", "gp-1499970072422", "<p>I \nreally\n feel like we need a better vocabulary and taxonomy of the difference pieces of intelligence.  IMO, the piece of intelligence that's required for manipulating humans requires being able to effectively model humans, which strikes me as somewhat incompatible with the very simplistic goal scenario--human's are \nvery\n messy around the goals they pursue.  It's possible, but it seems like it'd be using such different technologies in different parts of the AI that it would have to be intentionally designed by a human to do that.</p>", 1499970072], ["David&nbsp;German", "https://plus.google.com/111229345142780712481", "gp-1499970111429", "<p>If you aren't going to entertain robotic examples, then I agree you don't need to talk to roboticists.</p>", 1499970111]]